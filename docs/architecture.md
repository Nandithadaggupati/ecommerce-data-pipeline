# Data Pipeline Architecture

## Overview

This document describes the technical architecture of the e-commerce data pipeline, including system design, data flow, schema architecture, and deployment strategy.

---

## High-Level Architecture Diagram

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA SOURCES (CSVs)                                              â”‚
â”‚ customers.csv, products.csv, transactions.csv, items.csv        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Generation        â”‚ (generate_data.py)
â”‚ - Faker library        â”‚
â”‚ - 1000+ customers     â”‚
â”‚ - 500+ products       â”‚
â”‚ - 10000+ transactions â”‚
â”‚ - 30000+ items        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGING SCHEMA (PostgreSQL)         â”‚
â”‚ â”œâ”€ staging.customers               â”‚
â”‚ â”œâ”€ staging.products                â”‚
â”‚ â”œâ”€ staging.transactions            â”‚
â”‚ â””â”€ staging.transaction_items       â”‚
â”‚ ğŸ“Š Raw Data (No Constraints)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Quality Validation             â”‚
â”‚ (validate_data.py)                 â”‚
â”‚ âœ“ Completeness (nulls)             â”‚
â”‚ âœ“ Uniqueness (duplicates)          â”‚
â”‚ âœ“ Validity (ranges)                â”‚
â”‚ âœ“ Consistency (business rules)     â”‚
â”‚ âœ“ Referential Integrity (FKs)      â”‚
â”‚ ğŸ“ˆ Quality Score: 0â€“100             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION SCHEMA (PostgreSQL)      â”‚
â”‚ â”œâ”€ production.customers (3NF)      â”‚
â”‚ â”œâ”€ production.products (3NF)       â”‚
â”‚ â”œâ”€ production.transactions (3NF)   â”‚
â”‚ â””â”€ production.transaction_items    â”‚
â”‚ ğŸ”’ Cleansed + Validated Data        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformation Layer               â”‚
â”‚ (staging_to_production.py)         â”‚
â”‚ - Cleanse & deduplicate            â”‚
â”‚ - Apply business rules             â”‚
â”‚ - Enrich calculated fields         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WAREHOUSE SCHEMA (PostgreSQL)       â”‚
â”‚ Star Schema Design                 â”‚
â”‚                                    â”‚
â”‚ Dimensions:                        â”‚
â”‚ â”œâ”€ dim_customers (SCD Type 2)      â”‚
â”‚ â”œâ”€ dim_products (SCD Type 2)       â”‚
â”‚ â”œâ”€ dim_date                        â”‚
â”‚ â””â”€ dim_payment_method              â”‚
â”‚                                    â”‚
â”‚ Facts:                             â”‚
â”‚ â””â”€ fact_sales                      â”‚
â”‚                                    â”‚
â”‚ Aggregates:                        â”‚
â”‚ â”œâ”€ agg_daily_sales                 â”‚
â”‚ â”œâ”€ agg_product_sales               â”‚
â”‚ â””â”€ agg_customer_lifetime           â”‚
â”‚ ğŸ“Š Denormalized for BI              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BI Dashboards (Tableau/PowerBI)     â”‚
â”‚ â”œâ”€ KPI Overview                    â”‚
â”‚ â”œâ”€ Sales Trends                    â”‚
â”‚ â”œâ”€ Product Performance             â”‚
â”‚ â””â”€ Customer Analytics              â”‚
â”‚ ğŸ“ˆ Business Insights                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## Schema Architecture

### 1. Staging Schema (Raw Data Layer)

Purpose: Ingest raw data from source systems without transformation

Tables:
- staging.customers
- staging.products
- staging.transactions
- staging.transaction_items

Characteristics:
- Minimal constraints
- No foreign keys
- loaded_at timestamp
- Optimized for bulk inserts

Data Flow:
CSV â†’ ingest_to_staging.py â†’ PostgreSQL

---

### 2. Production Schema (Cleansed Data Layer)

Purpose: Store validated, cleansed business data

Tables:
- production.customers
- production.products
- production.transactions
- production.transaction_items

Characteristics:
- Fully normalized (3NF)
- Primary keys enforced
- Foreign keys enforced
- Surrogate keys
- Audit columns (is_active, created_at, updated_at)
- Indexed join columns

Cleansing Logic:

Customers:
- Remove duplicates
- Standardize names
- Fill missing fields with "Unknown"

Products:
- Fix invalid prices
- Ensure cost < price
- Fill missing supplier_id

Transactions:
- Validate dates
- Validate payment methods
- Handle NULL values

Transaction Items:
- Recalculate line totals
- Remove invalid quantities
- Validate discount range

---

### 3. Warehouse Schema (Dimensional Mart)

Purpose: Analytics-optimized star schema

#### Dimensions

dim_customers (SCD Type 2)
- customer_sk (PK)
- customer_id (business key)
- attributes
- is_current
- effective_date, end_date

dim_products (SCD Type 2)
- product_sk (PK)
- product_id (business key)
- attributes
- is_current
- effective_date, end_date

dim_date
- date_sk
- date attributes

dim_payment_method
- payment_method_sk
- attributes

#### Fact Table

fact_sales
- One row per transaction line item
- Foreign keys to all dimensions
- Measures: quantity, price, discount, totals

#### Aggregates

agg_daily_sales  
agg_product_sales  
agg_customer_lifetime  

---

## Data Flow Pipeline

Step 1: Data Generation  
generate_data.py â†’ CSVs + metadata

Step 2: Ingestion  
ingest_to_staging.py â†’ staging schema

Step 3: Quality Validation  
validate_data.py â†’ quality_report.json

Step 4: Transformation  
staging_to_production.py â†’ production schema

Step 5: Warehouse Loading  
load_warehouse.py â†’ dimensions, facts, aggregates

Step 6: Analytics & BI  
analytics_queries.sql â†’ Tableau / PowerBI

---

## Technology Stack

PostgreSQL 14+  
Python 3.9+  
Pandas  
SQLAlchemy  
Pytest  
Docker  
GitHub Actions  
Tableau / PowerBI  

---

## Performance Optimization

Staging:
- No indexes (fast load)

Production:
- Indexes on foreign keys

Warehouse:
- Indexes on surrogate keys
- Partitioned fact tables
### Production Schema
- Primary Keys (PK) on all tables
- Foreign Keys (FK) on transaction and transaction_item joins
- Indexes on:
  - customer_id
  - product_id
  - transaction_date

### Warehouse Schema
- Indexes on all Foreign Key (FK) columns
- Indexes on:
  - dim_date.date_id
  - dim_customers and dim_products business keys
- Clustered index on fact_sales.date_sk for time-series queries

---

## SCD Type 2 Implementation

**Slowly Changing Dimension Type 2:**  
Track all changes by creating new rows with versioning instead of overwriting data.

### Example: Customer Moves to a Different City

**Before Update**

| customer_sk | customer_id | first_name | city | is_current | effective_date | end_date |
|------------|-------------|------------|------|------------|----------------|----------|
| 1 | CUST001 | John | NYC | TRUE | 2023-01-01 | NULL |

**After Update (New City)**

| customer_sk | customer_id | first_name | city | is_current | effective_date | end_date |
|------------|-------------|------------|------|------------|----------------|----------|
| 1 | CUST001 | John | NYC | FALSE | 2023-01-01 | 2024-12-26 |
| 2 | CUST001 | John | LA  | TRUE  | 2024-12-27 | NULL |

### Benefits
- Historical tracking of customer and product changes
- Ability to analyze impact of attribute changes (e.g., price, location)
- Supports **as-of queries** (state of data at any past date)

---

## Error Handling & Monitoring

### Orchestrator Error Handling
1. **Step Failure:** Log error and continue if configured
2. **Database Connection:** Retry up to 3 times with 30-second delay
3. **Data Validation:** Flag quality issues without blocking pipeline
4. **Transaction Rollback:** TRUNCATE staging tables before reload (idempotent)

### Logging
- All pipeline steps log to `logs/pipeline.log`
- JSON reports generated per stage
- Central execution log with timestamps, status, and error details

### Monitoring Points
âœ“ Data generation: Record counts, date ranges  
âœ“ Ingestion: Rows loaded per table, NULL counts  
âœ“ Quality checks: Scores per dimension, violations  
âœ“ Transformation: Rows in vs rows out, rule violations  
âœ“ Warehouse: Dimension counts, fact counts, aggregate counts  

---

## Deployment Options

### Option 1: Local PostgreSQL + Python Virtual Environment

```bash
./setup.sh
export DB_HOST=localhost DB_PORT=5432 DB_NAME=ecommerce_db DB_USER=admin DB_PASSWORD=*****
python scripts/pipeline_orchestrator.py